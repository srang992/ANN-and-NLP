<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>Natural Language Processing and Artificial Neural Networks :: Smoothing</title>
  

  <link rel="icon" type="image/png" sizes="32x32" href="../_static/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../_static/img/favicon-16x16.png">
        <link rel="index" title="Index"
              href="../genindex.html"/>

  <link rel="stylesheet" href="../_static/css/insegel.css"/>

  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
        URL_ROOT:'',
        VERSION:'2021.02',
        LANGUAGE:'None',
        COLLAPSE_INDEX:false,
        FILE_SUFFIX:'.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
    };
  </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/mathjax/tex-chtml.js"></script>

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script>

</head>

<body>
  <div id="insegel-container">
    <header>
      <div id="logo-container">
          
          <a href="../index.html"><img src="../_static/img/logo.svg"></a>
          

      </div>
      <div id="project-container">
        <h1>Natural Language Processing and Artificial Neural Networks Documentation</h1>
      </div>
    </header>

    <div id="content-container">

      <div id="main-content-container">
        <div id="main-content-header">
          <h1><em>Smoothing</em></h1>
        </div>
        <div id="main-content">
          
  <div class="section" id="smoothing">
<h1><em>Smoothing</em><a class="headerlink" href="#smoothing" title="Permalink to this headline">¶</a></h1>
<p>Smoothing techniques in NLP are used to address scenarios related to determining probability / likelihood estimate of a sequence of words (say, a sentence) occuring together when one or more words individually (unigram) or N-grams such as bigram <span class="math notranslate nohighlight">\((\frac{w_{i}}{w_{i-1}})\)</span> or trigram <span class="math notranslate nohighlight">\((\frac{w_{i}}{w_{i-1}w_{i-2}})\)</span> in the given set have never occured in the past.</p>
<p>Here, we will go through a quick introduction to various different smoothing techniques used in NLP in addition to related formulas and examples. The following is the list of some of the smoothing techniques:</p>
<ol class="arabic simple">
<li><p><strong>Laplace smoothing: Another name for Laplace smoothing technique is add one smoothing.</strong></p></li>
<li><p><strong>Additive smoothing</strong></p></li>
<li><p><strong>Good-turing smoothing</strong></p></li>
<li><p><strong>Kneser-Ney smoothing</strong></p></li>
<li><p><strong>Katz smoothing</strong></p></li>
<li><p><strong>Church and Gale Smoothing</strong></p></li>
</ol>
<div class="section" id="why-smoothing-techniques">
<h2><em>Why Smoothing Techniques?</em><a class="headerlink" href="#why-smoothing-techniques" title="Permalink to this headline">¶</a></h2>
<p>Based on the training data set, what is the probability of “cats sleep” assuming bigram technique is used? Based on bigram technique, the probability of the sequence of words “cats sleep” can be calculated as the product of following:</p>
<p><span class="math notranslate nohighlight">\(P(cats sleep) = P(\frac{cats}{&lt;s&gt;})\times P(\frac{sleep}{cats})\times P(\frac{&lt;/s&gt;}{sleep})\)</span></p>
<p>you will notice that <span class="math notranslate nohighlight">\(P(\frac{sleep}{cats}) = 0\)</span>. Thus, the overall probability of occurrence of “cats sleep” would result in zero (0) value. However, the probability of occurrence of a sequence of words should not be zero at all.</p>
<p>This is where various different smoothing techniques come into the picture.</p>
<div class="section" id="laplace-add-one-smoothing">
<h3><em>Laplace(Add-One-Smoothing)</em><a class="headerlink" href="#laplace-add-one-smoothing" title="Permalink to this headline">¶</a></h3>
<p>In Laplace smoothing, 1 (one) is added to all the counts and thereafter, the probability is calculated. This is one of the most trivial smoothing techniques out of all the techniques.</p>
<p>Maximum likelihood estimate (MLE) of a word <span class="math notranslate nohighlight">\(w_{i}\)</span> occuring in a corpus can be calculated as the following. N is total number of words, and <span class="math notranslate nohighlight">\(count(w_{i})\)</span> is count of words for whose probability is required to be calculated.</p>
<p><strong>MLE</strong>: <span class="math notranslate nohighlight">\(P(w_{i}) = \frac{count(w_{i})}{N}\)</span></p>
<p>After applying Laplace smoothing, the following happens. Adding 1 leads to extra V observations.</p>
<p><strong>MLE</strong>: <span class="math notranslate nohighlight">\(P_{Laplace}(w_{i}) = \frac{count(w_{i}) + 1}{N + V}\)</span></p>
<p>Similarly, for N-grams (say, Bigram), MLE is calculated as the following:</p>
<p><span class="math notranslate nohighlight">\(P(\frac{w_{i}}{w_{i-1}}) = \frac{count(w_{i-1}, w_{i})}{count(w_{i-1})}\)</span></p>
<p>After applying Laplace smoothing, the following happens for N-grams (Bigram). Adding 1 leads to extra V observations.</p>
<p><strong>MLE</strong>: <span class="math notranslate nohighlight">\(P_{Laplace}(\frac{w_{i}}{w_{i-1}}) = \frac{count(w_{i-1}, w_{i}) + 1}{count(w_{i-1}) + V}\)</span></p>
</div>
<div class="section" id="additive-smoothing">
<h3><em>Additive Smoothing</em><a class="headerlink" href="#additive-smoothing" title="Permalink to this headline">¶</a></h3>
<p>This is very similar to “Add One” or Laplace smoothing. Instead of adding 1 as like in Laplace smoothing, a delta(δ) value is added. Thus, the formula to calculate probability using additive smoothing looks like following:</p>
<p><span class="math notranslate nohighlight">\(P(\frac{w_{i}}{w_{i-1}}) = \frac{count(w_{i-1}, w_{i}) + \delta}{count(w_{i-1}) + \delta|V|}\)</span></p>
</div>
<div class="section" id="good-turing-smoothing">
<h3><em>Good-Turing Smoothing</em><a class="headerlink" href="#good-turing-smoothing" title="Permalink to this headline">¶</a></h3>
<p>Good Turing Smoothing technique uses the frequencies of the count of occurrence of N-Grams for calculating the maximum likelihood estimate. For example, consider calculating the probability of a bigram (chatter/cats) from the corpus given above. Note that this bigram has never occurred in the corpus and thus, probability without smoothing would turn out to be zero. As per the Good-turing Smoothing, the probability will depend upon the following:</p>
<ul class="simple">
<li><p>In case, the bigram (chatter/cats) has never occurred in the corpus (which is the reality), the probability will depend upon the number of bigrams which occurred exactly one time and the total number of bigrams.</p></li>
<li><p>In case, the bigram has occurred in the corpus (for example, chatter/rats), the probability will depend upon number of bigrams which occurred more than one time of the current bigram (chatter/rats) (the value is 1 for chase/cats), total number of bigram which occurred same time as the current bigram (to/bigram) and total number of bigram.</p></li>
</ul>
<p>The following is the formula:</p>
<p>For the unknown N-grams, the following formula is used to calculate the probability:</p>
<p><span class="math notranslate nohighlight">\(P_{unknown}(\frac{w_{i}}{w_{i-1}}) = \frac{N_1}{N}\)</span></p>
<p>In above formula, N1 is count of N-grams which appeared one time and N is count of total number of N-grams</p>
<p>For the known N-grams, the following formula is used to calculate the probability:</p>
<p><span class="math notranslate nohighlight">\(P(\frac{w_{i}}{w_{i-1}}) = \frac{c*}{N}\)</span></p>
<p>where</p>
<div class="math notranslate nohighlight">
\[c*=(c + 1)\times\frac{N_{i+1}}{N_{c}}\]</div>
<p>In the above formula, c represents the count of occurrence of n-gram, <span class="math notranslate nohighlight">\(N_{c+1}\)</span> represents count of n-grams which occured for c + 1 times, :math:<a href="#id1"><span class="problematic" id="id2">`</span></a>N_{c}`represents count of n-grams which occured for c times and N represents total count of all n-grams.</p>
</div>
<div class="section" id="kneser-ney-smoothing">
<h3><em>Kneser-Ney Smoothing</em><a class="headerlink" href="#kneser-ney-smoothing" title="Permalink to this headline">¶</a></h3>
<p>In Good Turing smoothing, it is observed that the count of n-grams is discounted by a constant/abolute value such as 0.75. The same intuiton is applied for Kneser-Ney Smoothing where absolute discounting is applied to the count of n-grams in addition to adding the product of interpolation weight and probability of word to appear as novel continuation.</p>
<p><span class="math notranslate nohighlight">\(P_{Kneser-Ney}(\frac{w_{i}}{w_{i-1}}) = \frac{max(c(w_{i-1},w_{i} – d, 0))}{c(w_{i-1})} + \lambda(w_{i-1})*P_{continuation}(w_{i})\)</span></p>
<p>where λ is a normalizing constant which represents probability mass that have been discounted for higher order. The following represents how λ is calculated:</p>
<div class="math notranslate nohighlight">
\[\lambda(w_{i-1}) = \frac{d\times|c(w_{i-1},w_{i})|}{c(w_{i-1})}\]</div>
</div>
<div class="section" id="katz-smoothing">
<h3><em>Katz Smoothing</em><a class="headerlink" href="#katz-smoothing" title="Permalink to this headline">¶</a></h3>
<p>Good-turing technique is combined with interpolation. Outperforms Good-Turing by redistributing different probabilities to different unseen units.</p>
</div>
<div class="section" id="church-and-gale-smoothing">
<h3><em>Church and Gale Smoothing</em><a class="headerlink" href="#church-and-gale-smoothing" title="Permalink to this headline">¶</a></h3>
<p>Good-turing technique is combined with bucketing.</p>
<ul class="simple">
<li><p>Each n-gram is assigned to one of serveral buckets based on its frequency predicted from lower-order models.</p></li>
<li><p>Good-turing estimate is calculated for each bucket.</p></li>
</ul>
</div>
</div>
</div>


        </div>
      </div>

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search..." />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">

          
  
    
  
  
    <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="NLP%20Introduction.html"><em>What is NLP?</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Components%20of%20NLP.html"><em>Components of NLP</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Phases%20of%20NLP.html"><em>Phases of NLP</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Ambiguity.html"><em>Ambiguity</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Linguistic%20Resources.html"><em>Corpus</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Automata%20Theory.html"><em>Automata – What is it?</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Regular%20Expressions.html"><em>Regular Expressions</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Finite%20State%20Automata.html"><em>Finite State Automata</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Morphology.html"><em>What is Morphology</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Finite%20State%20Transducer.html"><em>Finite State Transducer</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="N-gram.html"><em>An Introduction to N-gram</em></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><em>Smoothing</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Morphological%20Parsing.html"><em>Morphological Parsing</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="POS%20Tagging.html"><em>Part of Speech(PoS) Tagging</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Semantics.html"><em>Semantic Analysis</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Word%20Sense%20Disambiguation.html"><em>Word Sense Disambiguation</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Discourse%20Processing.html"><em>Discourse Processing</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Inception.html"><em>Natural Language Inception</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="Information%20Retrieval.html"><em>Information Retrieval</em></a></li>
</ul>

  


        </div>

        

      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/docs/Smoothing.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            

            
        </ul>
        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/Autophagy/insegel">Insegel</a>

        </div>
    </div>

    <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

    <script type="text/javascript">
      $("#menu-toggle").click(function() {
        $("#menu-toggle").toggleClass("toggled");
        $("#side-menu-container").slideToggle(300);
      });
    </script>

</footer> 

</div>

</body>
</html>